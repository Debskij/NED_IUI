{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords, genesis\n",
    "import numpy as np\n",
    "import itertools"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1686\n",
      "Synset('continent.n.02')\n",
      "Synset('natal.n.01')\n",
      "Synset('union.n.02')\n",
      "Synset('earth.n.01')\n",
      "Synset('anchorage.n.03')\n",
      "Synset('athens.n.01')\n",
      "Synset('boreas.n.02')\n",
      "Synset('bunyan.n.02')\n",
      "Synset('caliphate.n.01')\n",
      "Synset('danube.n.01')\n",
      "Synset('death.n.06')\n",
      "Synset('downing_street.n.01')\n",
      "Synset('elizabeth.n.01')\n",
      "Synset('national_liberation_army.n.01')\n",
      "Synset('basque_homeland_and_freedom.n.01')\n",
      "Synset('fall.n.03')\n",
      "Synset('father.n.06')\n",
      "Synset('georgetown.n.02')\n",
      "Synset('ghana.n.01')\n",
      "Synset('indus.n.02')\n",
      "Synset('al-gama'a_al-islamiyya.n.01')\n",
      "Synset('al-jihad.n.01')\n",
      "Synset('kali.n.02')\n",
      "Synset('lashkar-e-taiba.n.01')\n",
      "Synset('mammon.n.02')\n",
      "Synset('mars.n.01')\n",
      "Synset('missouri.n.02')\n",
      "Synset('moon.n.01')\n",
      "Synset('neptune.n.02')\n",
      "Synset('paul.n.02')\n",
      "Synset('pluto.n.03')\n",
      "Synset('roosevelt.n.03')\n",
      "Synset('reign_of_terror.n.02')\n",
      "Synset('saturn.n.02')\n",
      "Synset('zion.n.01')\n",
      "Synset('shining_path.n.01')\n",
      "Synset('sun.n.01')\n",
      "Synset('tangier.n.01')\n",
      "Synset('liberation_tigers_of_tamil_eelam.n.01')\n",
      "Synset('uranus.n.02')\n",
      "Synset('valencia.n.02')\n",
      "Synset('vela.n.01')\n",
      "Synset('venus.n.01')\n",
      "Synset('yugoslavia.n.02')\n",
      "44\n",
      "7521\n"
     ]
    }
   ],
   "source": [
    "def find_highest_uncommon_ancestors(word, noun_only = True):\n",
    "    if noun_only:\n",
    "        synsets = wn.synsets(word, pos=wn.NOUN)\n",
    "    else:\n",
    "        # TODO\n",
    "        synsets = wn.synsets(word)\n",
    "    if len(synsets) <= 1:\n",
    "        return dict()\n",
    "    synsets_lca = {syn: set() for syn in synsets}\n",
    "    possible_relations = list(itertools.combinations(synsets, 2))\n",
    "    # print(len(possible_relations))\n",
    "    for a, b in possible_relations:\n",
    "        lch = a.lowest_common_hypernyms(b)\n",
    "        synsets_lca[a].add(lch[0])\n",
    "        synsets_lca[b].add(lch[0])\n",
    "    synsets_hua = dict()\n",
    "    for synset in synsets:\n",
    "        tree = synset.tree(lambda s:s.hypernyms())\n",
    "        current_entity = tree[0]\n",
    "        trees = tree[1:]\n",
    "        synsets_hua[synset] = highest_uncommon_ancestor(trees, synsets_lca[synset], current_entity)\n",
    "    return synsets_hua\n",
    "\n",
    "def highest_uncommon_ancestor(trees, synsets_lca, previous_entity):\n",
    "    ua_list = set()\n",
    "    for further_tree in trees:\n",
    "        while True:\n",
    "            next_entity = further_tree[0]\n",
    "            further_tree = further_tree[1:]\n",
    "            if len(further_tree) > 1:\n",
    "                ua_list.update(highest_uncommon_ancestor(further_tree, synsets_lca, previous_entity))\n",
    "                break\n",
    "            elif len(further_tree) == 1:\n",
    "                further_tree = further_tree[0]\n",
    "            if next_entity not in synsets_lca:\n",
    "                previous_entity = next_entity\n",
    "                if len(further_tree) == 0:\n",
    "                    ua_list.add(previous_entity)\n",
    "                    break\n",
    "            else:\n",
    "                ua_list.add(previous_entity)\n",
    "                break\n",
    "    return ua_list\n",
    "\n",
    "hua = {}\n",
    "lemmas_hua = {}\n",
    "rev_cnt = 0\n",
    "for l in wn.all_lemma_names(pos='n'):\n",
    "    try:\n",
    "        int(l)\n",
    "    except ValueError:\n",
    "        hua_dict = find_highest_uncommon_ancestors(l)\n",
    "        hua.update(hua_dict)\n",
    "        if len(list(hua_dict.items())):\n",
    "            lemmas_hua[l] = {}\n",
    "            for k, v in hua_dict.items():\n",
    "                for val in v:\n",
    "                    if val in lemmas_hua[l].keys():\n",
    "                        rev_cnt += 1\n",
    "                    lemmas_hua[l][val] = k\n",
    "print(rev_cnt)\n",
    "flatten_hua = set()\n",
    "cnt = 0\n",
    "for k, v in hua.items():\n",
    "    if len(k.examples()):\n",
    "        flatten_hua.add(k)\n",
    "        flatten_hua.update(v)\n",
    "        if len(v) == 0:\n",
    "            print(k)\n",
    "            cnt += 1\n",
    "print(cnt)\n",
    "print(len(flatten_hua))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cleaning(data):\n",
    "    clean = re.sub('<.*?>', ' ', str(data))\n",
    "#removes HTML tags\n",
    "    clean = re.sub('\\'.*?\\s',' ', clean)\n",
    "#removes all hanging letters afer apostrophes (s in it's)\n",
    "    clean = re.sub(r'http\\S+',' ', clean)\n",
    "#removes URLs\n",
    "    clean = re.sub('\\W+',' ', clean)\n",
    "#replacing the non alphanumeric characters\n",
    "    return html.unescape(clean)\n",
    "data['cleaned'] = data['review'].apply(cleaning)\n",
    "\n",
    "\n",
    "def tokenizing(data):\n",
    "    review = data['cleaned']\n",
    "#tokenizing is done\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    return tokens\n",
    "data['tokens'] = data.apply(tokenizing, axis=1)\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stops(data):\n",
    "    my_list = data['tokens']\n",
    "    meaningful_words = [w for w in my_list if not w in stop_words]           #stopwords are removed from the tokenized data\n",
    "    return (meaningful_words)\n",
    "data['tokens'] = data.apply(remove_stops, axis=1)\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatizing(data):\n",
    "    my_list = data['tokens']\n",
    "    lemmatized_list = [lemmatizer.lemmatize(word) for word in my_list]\n",
    "#lemmatizing is performed. It's more efficient than stemming.\n",
    "    return (lemmatized_list)\n",
    "data['tokens'] = data.apply(lemmatizing, axis=1)\n",
    "\n",
    "def rejoin_words(data):\n",
    "    my_list = data['tokens']\n",
    "    joined_words = ( \" \".join(my_list))\n",
    "#rejoins all stemmed words\n",
    "    return joined_words\n",
    "data['cleaned'] = data.apply(rejoin_words, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Synset('earth.n.01'): set(), Synset('earth.n.02'): {Synset('matter.n.03'), Synset('relation.n.01')}, Synset('land.n.04'): {Synset('land.n.04')}, Synset('earth.n.04'): {Synset('location.n.01')}, Synset('earth.n.05'): {Synset('matter.n.03'), Synset('relation.n.01')}, Synset('worldly_concern.n.01'): {Synset('attribute.n.02')}, Synset('ground.n.09'): {Synset('artifact.n.01')}}\n"
     ]
    }
   ],
   "source": [
    "print(find_highest_uncommon_ancestors('earth'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
